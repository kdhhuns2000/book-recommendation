{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36f14cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker import image_uris\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "# Get SageMaker role\n",
    "role = get_execution_role()\n",
    "\n",
    "# Using Factorization machines algorithm from sagemaker image\n",
    "container = image_uris.retrieve(\"factorization-machines\", region=\"ap-northeast-2\")\n",
    "\n",
    "bucket = 'gcu-ml2-005-bucket'\n",
    "sess = sagemaker.Session(\n",
    "    default_bucket = bucket\n",
    ")\n",
    "\n",
    "fm = Estimator(container,\n",
    "               role=role,\n",
    "               instance_count=1,\n",
    "               instance_type='ml.m5.large',\n",
    "               sagemaker_session=sess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d478b773",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = 's3://gcu-ml2-005-bucket/data/train.csv'\n",
    "test_data = 's3://gcu-ml2-005-bucket/data/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d97213da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = sagemaker.inputs.TrainingInput(train_data, content_type='text/csv')\n",
    "test_input = sagemaker.inputs.TrainingInput(test_data, content_type='text/csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08760f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: factorization-machines-2023-12-02-06-58-01-323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-02 06:58:01 Starting - Starting the training job...\n",
      "2023-12-02 06:58:17 Starting - Preparing the instances for training......\n",
      "2023-12-02 06:59:19 Downloading - Downloading input data......\n",
      "2023-12-02 07:00:04 Training - Downloading the training image.....\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.8/site-packages/mxnet/model.py:97: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if num_device is 1 and 'dist' not in kvstore:\u001b[0m\n",
      "\u001b[34m[12/02/2023 07:01:13 INFO 140319266056000] Reading default configuration from /opt/amazon/lib/python3.8/site-packages/algorithm/resources/default-conf.json: {'epochs': 1, 'mini_batch_size': '1000', 'use_bias': 'true', 'use_linear': 'true', 'bias_lr': '0.1', 'linear_lr': '0.001', 'factors_lr': '0.0001', 'bias_wd': '0.01', 'linear_wd': '0.001', 'factors_wd': '0.00001', 'bias_init_method': 'normal', 'bias_init_sigma': '0.01', 'linear_init_method': 'normal', 'linear_init_sigma': '0.01', 'factors_init_method': 'normal', 'factors_init_sigma': '0.001', 'batch_metrics_publish_interval': '500', '_data_format': 'record', '_kvstore': 'auto', '_learning_rate': '1.0', '_log_level': 'info', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_optimizer': 'adam', '_tuning_objective_metric': '', '_use_full_symbolic': 'true', '_wd': '1.0'}\u001b[0m\n",
      "\u001b[34m[12/02/2023 07:01:13 INFO 140319266056000] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'feature_dim': '6', 'mini_batch_size': '1000', 'num_factors': '8', 'predictor_type': 'binary_classifier'}\u001b[0m\n",
      "\u001b[34m[12/02/2023 07:01:13 INFO 140319266056000] Final configuration: {'epochs': 1, 'mini_batch_size': '1000', 'use_bias': 'true', 'use_linear': 'true', 'bias_lr': '0.1', 'linear_lr': '0.001', 'factors_lr': '0.0001', 'bias_wd': '0.01', 'linear_wd': '0.001', 'factors_wd': '0.00001', 'bias_init_method': 'normal', 'bias_init_sigma': '0.01', 'linear_init_method': 'normal', 'linear_init_sigma': '0.01', 'factors_init_method': 'normal', 'factors_init_sigma': '0.001', 'batch_metrics_publish_interval': '500', '_data_format': 'record', '_kvstore': 'auto', '_learning_rate': '1.0', '_log_level': 'info', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_optimizer': 'adam', '_tuning_objective_metric': '', '_use_full_symbolic': 'true', '_wd': '1.0', 'feature_dim': '6', 'num_factors': '8', 'predictor_type': 'binary_classifier'}\u001b[0m\n",
      "\u001b[34m[12/02/2023 07:01:13 WARNING 140319266056000] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34mProcess 7 is a worker.\u001b[0m\n",
      "\u001b[34m[12/02/2023 07:01:13 INFO 140319266056000] Using default worker.\u001b[0m\n",
      "\u001b[34m[12/02/2023 07:01:13 INFO 140319266056000] Checkpoint loading and saving are disabled.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701500473.2306623, \"EndTime\": 1701500473.2366521, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 5.785465240478516, \"count\": 1, \"min\": 5.785465240478516, \"max\": 5.785465240478516}}}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701500473.2368443, \"EndTime\": 1701500473.2368824, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"Meta\": \"init_train_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Total Batches Seen\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Max Records Seen Between Resets\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Max Batches Seen Between Resets\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Reset Count\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Records Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Batches Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 07:01:13 ERROR 140319266056000] Customer Error: Rows 1-1000 in file /opt/ml/input/data/train/train.csv have different fields than the expected size 7.\u001b[0m\n",
      "\n",
      "2023-12-02 07:01:27 Uploading - Uploading generated training model\n",
      "2023-12-02 07:01:27 Failed - Training job failed\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job factorization-machines-2023-12-02-06-58-01-323: Failed. Reason: ClientError: Rows 1-1000 in file /opt/ml/input/data/train/train.csv have different fields than the expected size 7., exit code: 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 12\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# 하이퍼파라미터 설정 및 모델 학습\u001b[39;00m\n\u001b[1;32m      7\u001b[0m fm\u001b[38;5;241m.\u001b[39mset_hyperparameters(feature_dim\u001b[38;5;241m=\u001b[39mfeature_dim,\n\u001b[1;32m      8\u001b[0m                        num_factors\u001b[38;5;241m=\u001b[39mnum_factors,\n\u001b[1;32m      9\u001b[0m                        predictor_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_classifier\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     10\u001b[0m                        mini_batch_size\u001b[38;5;241m=\u001b[39mmini_batch_size)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mfm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_input\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:311\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sagemaker/estimator.py:1322\u001b[0m, in \u001b[0;36mEstimatorBase.fit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjobs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_training_job)\n\u001b[1;32m   1321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 1322\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatest_training_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sagemaker/estimator.py:2605\u001b[0m, in \u001b[0;36m_TrainingJob.wait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   2603\u001b[0m \u001b[38;5;66;03m# If logs are requested, call logs_for_jobs.\u001b[39;00m\n\u001b[1;32m   2604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logs \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 2605\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogs_for_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2607\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39mwait_for_job(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_name)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sagemaker/session.py:4973\u001b[0m, in \u001b[0;36mSession.logs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   4952\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlogs_for_job\u001b[39m(\u001b[38;5;28mself\u001b[39m, job_name, wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, poll\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, log_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll\u001b[39m\u001b[38;5;124m\"\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   4953\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Display logs for a given training job, optionally tailing them until job is complete.\u001b[39;00m\n\u001b[1;32m   4954\u001b[0m \n\u001b[1;32m   4955\u001b[0m \u001b[38;5;124;03m    If the output is a tty or a Jupyter cell, it will be color-coded\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4971\u001b[0m \u001b[38;5;124;03m        exceptions.UnexpectedStatusException: If waiting and the training job fails.\u001b[39;00m\n\u001b[1;32m   4972\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4973\u001b[0m     \u001b[43m_logs_for_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mboto_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sagemaker/session.py:6897\u001b[0m, in \u001b[0;36m_logs_for_job\u001b[0;34m(boto_session, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   6894\u001b[0m             last_profiler_rule_statuses \u001b[38;5;241m=\u001b[39m profiler_rule_statuses\n\u001b[1;32m   6896\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 6897\u001b[0m     \u001b[43m_check_job_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTrainingJobStatus\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6898\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dot:\n\u001b[1;32m   6899\u001b[0m         \u001b[38;5;28mprint\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sagemaker/session.py:6950\u001b[0m, in \u001b[0;36m_check_job_status\u001b[0;34m(job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   6944\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCapacityError\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(reason):\n\u001b[1;32m   6945\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mCapacityError(\n\u001b[1;32m   6946\u001b[0m         message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   6947\u001b[0m         allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   6948\u001b[0m         actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   6949\u001b[0m     )\n\u001b[0;32m-> 6950\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mUnexpectedStatusException(\n\u001b[1;32m   6951\u001b[0m     message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   6952\u001b[0m     allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   6953\u001b[0m     actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   6954\u001b[0m )\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job factorization-machines-2023-12-02-06-58-01-323: Failed. Reason: ClientError: Rows 1-1000 in file /opt/ml/input/data/train/train.csv have different fields than the expected size 7., exit code: 2"
     ]
    }
   ],
   "source": [
    "# 하이퍼파라미터 설정\n",
    "feature_dim = 6  # 피처 차원\n",
    "num_factors = 8  # FM 알고리즘의 요인 수 설정\n",
    "mini_batch_size = 1000\n",
    "\n",
    "# 하이퍼파라미터 설정 및 모델 학습\n",
    "fm.set_hyperparameters(feature_dim=feature_dim,\n",
    "                       num_factors=num_factors,\n",
    "                       predictor_type='binary_classifier',\n",
    "                       mini_batch_size=mini_batch_size)\n",
    "\n",
    "fm.fit({'train': train_input, 'test': test_input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "185ffab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: factorization-machines-2023-12-01-16-09-15-342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-01 16:09:15 Starting - Starting the training job...\n",
      "2023-12-01 16:09:31 Starting - Preparing the instances for training.........\n",
      "2023-12-01 16:10:47 Downloading - Downloading input data...\n",
      "2023-12-01 16:11:23 Training - Downloading the training image..................\n",
      "2023-12-01 16:14:44 Training - Training image download completed. Training in progress..\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.8/site-packages/mxnet/model.py:97: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if num_device is 1 and 'dist' not in kvstore:\u001b[0m\n",
      "\u001b[34m[12/01/2023 16:14:51 INFO 140377310648128] Reading default configuration from /opt/amazon/lib/python3.8/site-packages/algorithm/resources/default-conf.json: {'epochs': 1, 'mini_batch_size': '1000', 'use_bias': 'true', 'use_linear': 'true', 'bias_lr': '0.1', 'linear_lr': '0.001', 'factors_lr': '0.0001', 'bias_wd': '0.01', 'linear_wd': '0.001', 'factors_wd': '0.00001', 'bias_init_method': 'normal', 'bias_init_sigma': '0.01', 'linear_init_method': 'normal', 'linear_init_sigma': '0.01', 'factors_init_method': 'normal', 'factors_init_sigma': '0.001', 'batch_metrics_publish_interval': '500', '_data_format': 'record', '_kvstore': 'auto', '_learning_rate': '1.0', '_log_level': 'info', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_optimizer': 'adam', '_tuning_objective_metric': '', '_use_full_symbolic': 'true', '_wd': '1.0'}\u001b[0m\n",
      "\u001b[34m[12/01/2023 16:14:51 INFO 140377310648128] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {}\u001b[0m\n",
      "\u001b[34m[12/01/2023 16:14:51 ERROR 140377310648128] Customer Error: No value(s) were specified for 'feature_dim', 'num_factors', 'predictor_type' which are required hyperparameter(s) (caused by ValidationError)\u001b[0m\n",
      "\n",
      "2023-12-01 16:15:06 Uploading - Uploading generated training model\n",
      "2023-12-01 16:15:06 Failed - Training job failed\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job factorization-machines-2023-12-01-16-09-15-342: Failed. Reason: ClientError: No value(s) were specified for 'feature_dim', 'num_factors', 'predictor_type' which are required hyperparameter(s) (caused by ValidationError), exit code: 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:311\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sagemaker/estimator.py:1322\u001b[0m, in \u001b[0;36mEstimatorBase.fit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjobs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_training_job)\n\u001b[1;32m   1321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 1322\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatest_training_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sagemaker/estimator.py:2605\u001b[0m, in \u001b[0;36m_TrainingJob.wait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   2603\u001b[0m \u001b[38;5;66;03m# If logs are requested, call logs_for_jobs.\u001b[39;00m\n\u001b[1;32m   2604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logs \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 2605\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogs_for_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2607\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39mwait_for_job(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_name)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sagemaker/session.py:4973\u001b[0m, in \u001b[0;36mSession.logs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   4952\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlogs_for_job\u001b[39m(\u001b[38;5;28mself\u001b[39m, job_name, wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, poll\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, log_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll\u001b[39m\u001b[38;5;124m\"\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   4953\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Display logs for a given training job, optionally tailing them until job is complete.\u001b[39;00m\n\u001b[1;32m   4954\u001b[0m \n\u001b[1;32m   4955\u001b[0m \u001b[38;5;124;03m    If the output is a tty or a Jupyter cell, it will be color-coded\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4971\u001b[0m \u001b[38;5;124;03m        exceptions.UnexpectedStatusException: If waiting and the training job fails.\u001b[39;00m\n\u001b[1;32m   4972\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4973\u001b[0m     \u001b[43m_logs_for_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mboto_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sagemaker/session.py:6897\u001b[0m, in \u001b[0;36m_logs_for_job\u001b[0;34m(boto_session, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   6894\u001b[0m             last_profiler_rule_statuses \u001b[38;5;241m=\u001b[39m profiler_rule_statuses\n\u001b[1;32m   6896\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 6897\u001b[0m     \u001b[43m_check_job_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTrainingJobStatus\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6898\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dot:\n\u001b[1;32m   6899\u001b[0m         \u001b[38;5;28mprint\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sagemaker/session.py:6950\u001b[0m, in \u001b[0;36m_check_job_status\u001b[0;34m(job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   6944\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCapacityError\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(reason):\n\u001b[1;32m   6945\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mCapacityError(\n\u001b[1;32m   6946\u001b[0m         message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   6947\u001b[0m         allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   6948\u001b[0m         actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   6949\u001b[0m     )\n\u001b[0;32m-> 6950\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mUnexpectedStatusException(\n\u001b[1;32m   6951\u001b[0m     message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   6952\u001b[0m     allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   6953\u001b[0m     actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   6954\u001b[0m )\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job factorization-machines-2023-12-01-16-09-15-342: Failed. Reason: ClientError: No value(s) were specified for 'feature_dim', 'num_factors', 'predictor_type' which are required hyperparameter(s) (caused by ValidationError), exit code: 2"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc4480b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./train_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad70bc5c",
   "metadata": {},
   "source": [
    "## Model NCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb5850bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20695\n",
      "186\n"
     ]
    }
   ],
   "source": [
    "%store -r n_user\n",
    "%store -r n_item\n",
    "\n",
    "print(n_user)\n",
    "print(n_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "894f478c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "# get current SageMaker session's execution role and default bucket name\n",
    "bucket = 'gcu-ml2-005-bucket'\n",
    "sess = sagemaker.Session(\n",
    "    default_bucket = bucket\n",
    ")\n",
    "\n",
    "role = get_execution_role()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07e27fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\u001b[39;49;00m\r\n",
      "\u001b[33m SPDX-License-Identifier: MIT-0\u001b[39;49;00m\r\n",
      "\u001b[33m \u001b[39;49;00m\r\n",
      "\u001b[33m Permission is hereby granted, free of charge, to any person obtaining a copy of this\u001b[39;49;00m\r\n",
      "\u001b[33m software and associated documentation files (the \"Software\"), to deal in the Software\u001b[39;49;00m\r\n",
      "\u001b[33m without restriction, including without limitation the rights to use, copy, modify,\u001b[39;49;00m\r\n",
      "\u001b[33m merge, publish, distribute, sublicense, and/or sell copies of the Software, and to\u001b[39;49;00m\r\n",
      "\u001b[33m permit persons to whom the Software is furnished to do so.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED,\u001b[39;49;00m\r\n",
      "\u001b[33m INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A\u001b[39;49;00m\r\n",
      "\u001b[33m PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT\u001b[39;49;00m\r\n",
      "\u001b[33m HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\u001b[39;49;00m\r\n",
      "\u001b[33m OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\u001b[39;49;00m\r\n",
      "\u001b[33m SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m\"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m# for data processing\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_load_training_data\u001b[39;49;00m(base_dir):\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\" load training data \"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\r\n",
      "    df_train = np.load(os.path.join(base_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mtrain.npy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\u001b[37m\u001b[39;49;00m\r\n",
      "    user_train, item_train, y_train = np.split(np.transpose(df_train).flatten(), \u001b[34m3\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m user_train, item_train, y_train\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mbatch_generator\u001b[39;49;00m(x, y, batch_size, n_batch, shuffle, user_dim, item_dim):\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\" batch generator to supply data for training and testing \"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "    user_df, item_df = x\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "    counter = \u001b[34m0\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\r\n",
      "    training_index = np.arange(user_df.shape[\u001b[34m0\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "    \u001b[34mif\u001b[39;49;00m shuffle:\u001b[37m\u001b[39;49;00m\r\n",
      "        np.random.shuffle(training_index)\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "    \u001b[34mwhile\u001b[39;49;00m \u001b[34mTrue\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\r\n",
      "        batch_index = training_index[batch_size*counter:batch_size*(counter+\u001b[34m1\u001b[39;49;00m)]\u001b[37m\u001b[39;49;00m\r\n",
      "        user_batch = tf.one_hot(user_df[batch_index], depth=user_dim)\u001b[37m\u001b[39;49;00m\r\n",
      "        item_batch = tf.one_hot(item_df[batch_index], depth=item_dim)\u001b[37m\u001b[39;49;00m\r\n",
      "        y_batch = y[batch_index]\u001b[37m\u001b[39;49;00m\r\n",
      "        counter += \u001b[34m1\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\r\n",
      "        \u001b[34myield\u001b[39;49;00m [user_batch, item_batch], y_batch\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "        \u001b[34mif\u001b[39;49;00m counter == n_batch:\u001b[37m\u001b[39;49;00m\r\n",
      "            \u001b[34mif\u001b[39;49;00m shuffle:\u001b[37m\u001b[39;49;00m\r\n",
      "                np.random.shuffle(training_index)\u001b[37m\u001b[39;49;00m\r\n",
      "            counter = \u001b[34m0\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m# network\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_get_user_embedding_layers\u001b[39;49;00m(inputs, emb_dim):\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\" create user embeddings \"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\r\n",
      "    user_gmf_emb = tf.keras.layers.Dense(emb_dim, activation=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(inputs)\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "    user_mlp_emb = tf.keras.layers.Dense(emb_dim, activation=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(inputs)\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m user_gmf_emb, user_mlp_emb\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_get_item_embedding_layers\u001b[39;49;00m(inputs, emb_dim):\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\" create item embeddings \"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\r\n",
      "    item_gmf_emb = tf.keras.layers.Dense(emb_dim, activation=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(inputs)\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "    item_mlp_emb = tf.keras.layers.Dense(emb_dim, activation=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(inputs)\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m item_gmf_emb, item_mlp_emb\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_gmf\u001b[39;49;00m(user_emb, item_emb):\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\" general matrix factorization branch \"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\r\n",
      "    gmf_mat = tf.keras.layers.Multiply()([user_emb, item_emb])\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m gmf_mat\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_mlp\u001b[39;49;00m(user_emb, item_emb, dropout_rate):\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\" multi-layer perceptron branch \"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32madd_layer\u001b[39;49;00m(dim, input_layer, dropout_rate):\u001b[37m\u001b[39;49;00m\r\n",
      "        hidden_layer = tf.keras.layers.Dense(dim, activation=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(input_layer)\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "        \u001b[34mif\u001b[39;49;00m dropout_rate:\u001b[37m\u001b[39;49;00m\r\n",
      "            dropout_layer = tf.keras.layers.Dropout(dropout_rate)(hidden_layer)\u001b[37m\u001b[39;49;00m\r\n",
      "            \u001b[34mreturn\u001b[39;49;00m dropout_layer\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m hidden_layer\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "    concat_layer = tf.keras.layers.Concatenate()([user_emb, item_emb])\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "    dropout_l1 = tf.keras.layers.Dropout(dropout_rate)(concat_layer)\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "    dense_layer_1 = add_layer(\u001b[34m64\u001b[39;49;00m, dropout_l1, dropout_rate)\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "    dense_layer_2 = add_layer(\u001b[34m32\u001b[39;49;00m, dense_layer_1, dropout_rate)\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "    dense_layer_3 = add_layer(\u001b[34m16\u001b[39;49;00m, dense_layer_2, \u001b[34mNone\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "    dense_layer_4 = add_layer(\u001b[34m8\u001b[39;49;00m, dense_layer_3, \u001b[34mNone\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m dense_layer_4\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_neuCF\u001b[39;49;00m(gmf, mlp, dropout_rate):\u001b[37m\u001b[39;49;00m\r\n",
      "    concat_layer = tf.keras.layers.Concatenate()([gmf, mlp])\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "    output_layer = tf.keras.layers.Dense(\u001b[34m1\u001b[39;49;00m, activation=\u001b[33m'\u001b[39;49;00m\u001b[33msigmoid\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(concat_layer)\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m output_layer\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mbuild_graph\u001b[39;49;00m(user_dim, item_dim, dropout_rate=\u001b[34m0.25\u001b[39;49;00m):\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\" neural collaborative filtering model \"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "    user_input = tf.keras.Input(shape=(user_dim))\u001b[37m\u001b[39;49;00m\r\n",
      "    item_input = tf.keras.Input(shape=(item_dim))\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "    \u001b[37m# create embedding layers\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\r\n",
      "    user_gmf_emb, user_mlp_emb = _get_user_embedding_layers(user_input, \u001b[34m32\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\r\n",
      "    item_gmf_emb, item_mlp_emb = _get_item_embedding_layers(item_input, \u001b[34m32\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "    \u001b[37m# general matrix factorization\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\r\n",
      "    gmf = _gmf(user_gmf_emb, item_gmf_emb)\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "    \u001b[37m# multi layer perceptron\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\r\n",
      "    mlp = _mlp(user_mlp_emb, item_mlp_emb, dropout_rate)\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "    \u001b[37m# output\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\r\n",
      "    output = _neuCF(gmf, mlp, dropout_rate)\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "    \u001b[37m# create the model\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\r\n",
      "    model = tf.keras.Model(inputs=[user_input, item_input], outputs=output)\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m model\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel\u001b[39;49;00m(x_train, y_train, n_user, n_item, num_epoch, batch_size):\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "    num_batch = np.ceil(x_train[\u001b[34m0\u001b[39;49;00m].shape[\u001b[34m0\u001b[39;49;00m]/batch_size)\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "    \u001b[37m# build graph\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\r\n",
      "    model = build_graph(n_user, n_item)\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "    \u001b[37m# compile and train\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\r\n",
      "    optimizer = tf.keras.optimizers.Adam(learning_rate=\u001b[34m1e-3\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "    model.compile(optimizer=optimizer,\u001b[37m\u001b[39;49;00m\r\n",
      "                  loss=tf.keras.losses.BinaryCrossentropy(),\u001b[37m\u001b[39;49;00m\r\n",
      "                  metrics=[\u001b[33m'\u001b[39;49;00m\u001b[33maccuracy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "    model.fit_generator(\u001b[37m\u001b[39;49;00m\r\n",
      "        generator=batch_generator(\u001b[37m\u001b[39;49;00m\r\n",
      "            x=x_train, y=y_train,\u001b[37m\u001b[39;49;00m\r\n",
      "            batch_size=batch_size, n_batch=num_batch,\u001b[37m\u001b[39;49;00m\r\n",
      "            shuffle=\u001b[34mTrue\u001b[39;49;00m, user_dim=n_user, item_dim=n_item),\u001b[37m\u001b[39;49;00m\r\n",
      "        epochs=num_epoch,\u001b[37m\u001b[39;49;00m\r\n",
      "        steps_per_epoch=num_batch,\u001b[37m\u001b[39;49;00m\r\n",
      "        verbose=\u001b[34m2\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\r\n",
      "    )\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m model\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_parse_args\u001b[39;49;00m():\u001b[37m\u001b[39;49;00m\r\n",
      "    parser = argparse.ArgumentParser()\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--sm-model-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\u001b[37m\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAINING\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\u001b[37m\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mlist\u001b[39;49;00m, default=json.loads(os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_HOSTS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)))\u001b[37m\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\u001b[37m\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m3\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--batch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m256\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--n_user\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--n_item\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m parser.parse_known_args()\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\r\n",
      "    args, unknown = _parse_args()\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "    \u001b[37m# load data\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\r\n",
      "    user_train, item_train, train_labels = _load_training_data(args.train)\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "    \u001b[37m# build model\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\r\n",
      "    ncf_model = model(\u001b[37m\u001b[39;49;00m\r\n",
      "        x_train=[user_train, item_train],\u001b[37m\u001b[39;49;00m\r\n",
      "        y_train=train_labels,\u001b[37m\u001b[39;49;00m\r\n",
      "        n_user=args.n_user,\u001b[37m\u001b[39;49;00m\r\n",
      "        n_item=args.n_item,\u001b[37m\u001b[39;49;00m\r\n",
      "        num_epoch=args.epochs,\u001b[37m\u001b[39;49;00m\r\n",
      "        batch_size=args.batch_size\u001b[37m\u001b[39;49;00m\r\n",
      "    )\u001b[37m\u001b[39;49;00m\r\n",
      "\u001b[37m\u001b[39;49;00m\r\n",
      "    \u001b[34mif\u001b[39;49;00m args.current_host == args.hosts[\u001b[34m0\u001b[39;49;00m]:\u001b[37m\u001b[39;49;00m\r\n",
      "        \u001b[37m# save model to an S3 directory with version number '00000001'\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\r\n",
      "        ncf_model.save(os.path.join(args.sm_model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33m000000001\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), \u001b[33m'\u001b[39;49;00m\u001b[33mneural_collaborative_filtering.h5\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\r\n"
     ]
    }
   ],
   "source": [
    "# inspect the training script using `pygmentize` magic\n",
    "!pygmentize 'ncf.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53336eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify training instance type and model hyperparameters\n",
    "# note that for the demo purpose, the number of epoch is set to 1\n",
    "\n",
    "num_of_instance = 1                 # number of instance to use for training\n",
    "instance_type = 'ml.m5.large'     # type of instance to use for training\n",
    "\n",
    "training_script = 'ncf.py'\n",
    "\n",
    "training_parameters = {\n",
    "    'epochs': 1,\n",
    "    'batch_size': 256, \n",
    "    'n_user': n_user, \n",
    "    'n_item': n_item\n",
    "}\n",
    "\n",
    "# training framework specs\n",
    "tensorflow_version = '2.1.0'\n",
    "python_version = 'py3'\n",
    "distributed_training_spec = {'parameter_server': {'enabled': True}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ca1cb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " training_data_uri = os.path.join(f's3://{bucket_name}', 'data') \n",
    " print(training_data_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c0dc91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Amazon SageMaker Debugger does not currently support Parameter Server distribution\n",
      "INFO:sagemaker:Amazon SageMaker Debugger does not currently support Parameter Server distribution\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: tensorflow-training-2023-12-02-07-11-19-744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n",
      "2023-12-02 07:11:20 Starting - Starting the training job...\n",
      "2023-12-02 07:11:39 Starting - Preparing the instances for training......\n",
      "2023-12-02 07:12:43 Downloading - Downloading input data......\n",
      "2023-12-02 07:13:38 Training - Training image download completed. Training in progress..\u001b[34m2023-12-02 07:13:44,766 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[34m2023-12-02 07:13:44,775 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-12-02 07:13:44,989 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-12-02 07:13:45,007 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-12-02 07:13:45,027 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-12-02 07:13:45,041 sagemaker-containers INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_parameter_server_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": 256,\n",
      "        \"epochs\": 1,\n",
      "        \"model_dir\": \"s3://sagemaker-ap-northeast-2-629515838455/tensorflow-training-2023-12-02-07-11-19-744/model\",\n",
      "        \"n_item\": 186,\n",
      "        \"n_user\": 20695\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"tensorflow-training-2023-12-02-07-11-19-744\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-ap-northeast-2-629515838455/tensorflow-training-2023-12-02-07-11-19-744/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"ncf\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 2,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m5.large\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.large\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"ncf.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch_size\":256,\"epochs\":1,\"model_dir\":\"s3://sagemaker-ap-northeast-2-629515838455/tensorflow-training-2023-12-02-07-11-19-744/model\",\"n_item\":186,\"n_user\":20695}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=ncf.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_parameter_server_enabled\":true}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.large\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=ncf\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=2\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-ap-northeast-2-629515838455/tensorflow-training-2023-12-02-07-11-19-744/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_parameter_server_enabled\":true},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch_size\":256,\"epochs\":1,\"model_dir\":\"s3://sagemaker-ap-northeast-2-629515838455/tensorflow-training-2023-12-02-07-11-19-744/model\",\"n_item\":186,\"n_user\":20695},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"tensorflow-training-2023-12-02-07-11-19-744\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-northeast-2-629515838455/tensorflow-training-2023-12-02-07-11-19-744/source/sourcedir.tar.gz\",\"module_name\":\"ncf\",\"network_interface_name\":\"eth0\",\"num_cpus\":2,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.large\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"ncf.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch_size\",\"256\",\"--epochs\",\"1\",\"--model_dir\",\"s3://sagemaker-ap-northeast-2-629515838455/tensorflow-training-2023-12-02-07-11-19-744/model\",\"--n_item\",\"186\",\"--n_user\",\"20695\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH_SIZE=256\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_DIR=s3://sagemaker-ap-northeast-2-629515838455/tensorflow-training-2023-12-02-07-11-19-744/model\u001b[0m\n",
      "\u001b[34mSM_HP_N_ITEM=186\u001b[0m\n",
      "\u001b[34mSM_HP_N_USER=20695\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/usr/bin/python3 ncf.py --batch_size 256 --epochs 1 --model_dir s3://sagemaker-ap-northeast-2-629515838455/tensorflow-training-2023-12-02-07-11-19-744/model --n_item 186 --n_user 20695\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From ncf.py:168: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mPlease use Model.fit, which supports generators.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\u001b[0m\n",
      "\u001b[34mTrain for 1388.0 steps\u001b[0m\n",
      "\n",
      "2023-12-02 07:14:49 Uploading - Uploading generated training model\u001b[34m1388/1388 - 58s - loss: 0.4483 - accuracy: 0.8318\u001b[0m\n",
      "\u001b[34m2023-12-02 07:14:44.924364: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\u001b[0m\n",
      "\u001b[34m2023-12-02 07:14:45.450791: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mIf using Keras pass *_constraint arguments to layers.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mIf using Keras pass *_constraint arguments to layers.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Assets written to: /opt/ml/model/000000001/assets\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Assets written to: /opt/ml/model/000000001/assets\u001b[0m\n",
      "\u001b[34m2023-12-02 07:14:46,355 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2023-12-02 07:15:00 Completed - Training job completed\n",
      "Training seconds: 138\n",
      "Billable seconds: 138\n"
     ]
    }
   ],
   "source": [
    "# initiate the training job using Tensorflow estimator\n",
    "ncf_estimator = TensorFlow(\n",
    "    entry_point=training_script,\n",
    "    role=role,\n",
    "    instance_count=num_of_instance,\n",
    "    instance_type=instance_type,\n",
    "    framework_version=tensorflow_version,\n",
    "    py_version=python_version,\n",
    "    distribution=distributed_training_spec,\n",
    "    hyperparameters=training_parameters\n",
    ")\n",
    "\n",
    "ncf_estimator.fit(train_data_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7499d5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.tensorflow.model:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating model with name: book-recommend-model-005\n",
      "INFO:sagemaker:Creating endpoint-config with name book-endpoint-ncf-005\n",
      "INFO:sagemaker:Creating endpoint with name book-endpoint-ncf-005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---!"
     ]
    }
   ],
   "source": [
    "endpoint_name = 'book-endpoint-ncf-005'\n",
    "model_name = 'book-recommend-model-005'\n",
    "\n",
    "predictor = ncf_estimator.deploy(\n",
    "    initial_instance_count=1, \n",
    "    instance_type=\"ml.m5.large\", \n",
    "    endpoint_name=endpoint_name,\n",
    "    model_name=model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba0dbf81",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SageMaker' object has no attribute 'invoke_endpoint'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m sagemaker \u001b[38;5;241m=\u001b[39m boto3\u001b[38;5;241m.\u001b[39mclient(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msagemaker\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# 추론 요청 보내기\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43msagemaker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke_endpoint\u001b[49m(\n\u001b[1;32m     19\u001b[0m     EndpointName\u001b[38;5;241m=\u001b[39mendpoint_name,\n\u001b[1;32m     20\u001b[0m     ContentType\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# 추론 결과 가져오기\u001b[39;00m\n\u001b[1;32m     24\u001b[0m result \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBody\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mdecode())\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/botocore/client.py:888\u001b[0m, in \u001b[0;36mBaseClient.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    886\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m event_response\n\u001b[0;32m--> 888\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    889\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    890\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SageMaker' object has no attribute 'invoke_endpoint'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00337a3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
